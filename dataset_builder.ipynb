{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17e30dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\link5\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset_builder\n",
    "\n",
    "builder = load_dataset_builder(\n",
    "    \"code-search-net/code_search_net\", trust_remote_code=True\n",
    ")\n",
    "builder.download_and_prepare()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a99bc806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on CodeSearchNet in module datasets_modules.datasets.code-search-net--code_search_net.8f2524e6b62f65af5f5d65c53715c654db7b08dc93e0b7bcce2ab2f286a75be1.code_search_net object:\n",
      "\n",
      "class CodeSearchNet(datasets.builder.GeneratorBasedBuilder)\n",
      " |  CodeSearchNet(\n",
      " |      cache_dir: Optional[str] = None,\n",
      " |      dataset_name: Optional[str] = None,\n",
      " |      config_name: Optional[str] = None,\n",
      " |      hash: Optional[str] = None,\n",
      " |      base_path: Optional[str] = None,\n",
      " |      info: Optional[datasets.info.DatasetInfo] = None,\n",
      " |      features: Optional[datasets.features.features.Features] = None,\n",
      " |      token: Union[bool, str, NoneType] = None,\n",
      " |      repo_id: Optional[str] = None,\n",
      " |      data_files: Union[str, list, dict, datasets.data_files.DataFilesDict, NoneType] = None,\n",
      " |      data_dir: Optional[str] = None,\n",
      " |      storage_options: Optional[dict] = None,\n",
      " |      writer_batch_size: Optional[int] = None,\n",
      " |      **config_kwargs\n",
      " |  )\n",
      " |\n",
      " |  \"CodeSearchNet corpus: proxy dataset for semantic code search.\n",
      " |\n",
      " |  Method resolution order:\n",
      " |      CodeSearchNet\n",
      " |      datasets.builder.GeneratorBasedBuilder\n",
      " |      datasets.builder.DatasetBuilder\n",
      " |      builtins.object\n",
      " |\n",
      " |  Data and other attributes defined here:\n",
      " |\n",
      " |  BUILDER_CONFIGS = [BuilderConfig(name='all', version=1.0.0, data_di......\n",
      " |\n",
      " |  DEFAULT_CONFIG_NAME = 'all'\n",
      " |\n",
      " |  VERSION = 1.0.0\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from datasets.builder.DatasetBuilder:\n",
      " |\n",
      " |  __getstate__(self)\n",
      " |      Helper for pickle.\n",
      " |\n",
      " |  __init__(\n",
      " |      self,\n",
      " |      cache_dir: Optional[str] = None,\n",
      " |      dataset_name: Optional[str] = None,\n",
      " |      config_name: Optional[str] = None,\n",
      " |      hash: Optional[str] = None,\n",
      " |      base_path: Optional[str] = None,\n",
      " |      info: Optional[datasets.info.DatasetInfo] = None,\n",
      " |      features: Optional[datasets.features.features.Features] = None,\n",
      " |      token: Union[bool, str, NoneType] = None,\n",
      " |      repo_id: Optional[str] = None,\n",
      " |      data_files: Union[str, list, dict, datasets.data_files.DataFilesDict, NoneType] = None,\n",
      " |      data_dir: Optional[str] = None,\n",
      " |      storage_options: Optional[dict] = None,\n",
      " |      writer_batch_size: Optional[int] = None,\n",
      " |      **config_kwargs\n",
      " |  )\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |\n",
      " |  __setstate__(self, d)\n",
      " |\n",
      " |  as_dataset(\n",
      " |      self,\n",
      " |      split: Optional[datasets.splits.Split] = None,\n",
      " |      run_post_process=True,\n",
      " |      verification_mode: Union[datasets.utils.info_utils.VerificationMode, str, NoneType] = None,\n",
      " |      in_memory=False\n",
      " |  ) -> Union[datasets.arrow_dataset.Dataset, datasets.dataset_dict.DatasetDict]\n",
      " |      Return a Dataset for the specified split.\n",
      " |\n",
      " |      Args:\n",
      " |          split (`datasets.Split`):\n",
      " |              Which subset of the data to return.\n",
      " |          run_post_process (`bool`, defaults to `True`):\n",
      " |              Whether to run post-processing dataset transforms and/or add\n",
      " |              indexes.\n",
      " |          verification_mode ([`VerificationMode`] or `str`, defaults to `BASIC_CHECKS`):\n",
      " |              Verification mode determining the checks to run on the\n",
      " |              downloaded/processed dataset information (checksums/size/splits/...).\n",
      " |\n",
      " |              <Added version=\"2.9.1\"/>\n",
      " |          in_memory (`bool`, defaults to `False`):\n",
      " |              Whether to copy the data in-memory.\n",
      " |\n",
      " |      Returns:\n",
      " |          datasets.Dataset\n",
      " |\n",
      " |      Example:\n",
      " |\n",
      " |      ```py\n",
      " |      >>> from datasets import load_dataset_builder\n",
      " |      >>> builder = load_dataset_builder('cornell-movie-review-data/rotten_tomatoes')\n",
      " |      >>> builder.download_and_prepare()\n",
      " |      >>> ds = builder.as_dataset(split='train')\n",
      " |      >>> ds\n",
      " |      Dataset({\n",
      " |          features: ['text', 'label'],\n",
      " |          num_rows: 8530\n",
      " |      })\n",
      " |      ```\n",
      " |\n",
      " |  as_streaming_dataset(\n",
      " |      self,\n",
      " |      split: Optional[str] = None,\n",
      " |      base_path: Optional[str] = None\n",
      " |  ) -> Union[dict[str, datasets.iterable_dataset.IterableDataset], datasets.iterable_dataset.IterableDataset]\n",
      " |\n",
      " |  download_and_prepare(\n",
      " |      self,\n",
      " |      output_dir: Optional[str] = None,\n",
      " |      download_config: Optional[datasets.download.download_config.DownloadConfig] = None,\n",
      " |      download_mode: Union[datasets.download.download_manager.DownloadMode, str, NoneType] = None,\n",
      " |      verification_mode: Union[datasets.utils.info_utils.VerificationMode, str, NoneType] = None,\n",
      " |      dl_manager: Optional[datasets.download.download_manager.DownloadManager] = None,\n",
      " |      base_path: Optional[str] = None,\n",
      " |      file_format: str = 'arrow',\n",
      " |      max_shard_size: Union[str, int, NoneType] = None,\n",
      " |      num_proc: Optional[int] = None,\n",
      " |      storage_options: Optional[dict] = None,\n",
      " |      **download_and_prepare_kwargs\n",
      " |  )\n",
      " |      Downloads and prepares dataset for reading.\n",
      " |\n",
      " |      Args:\n",
      " |          output_dir (`str`, *optional*):\n",
      " |              Output directory for the dataset.\n",
      " |              Default to this builder's `cache_dir`, which is inside `~/.cache/huggingface/datasets` by default.\n",
      " |\n",
      " |              <Added version=\"2.5.0\"/>\n",
      " |          download_config (`DownloadConfig`, *optional*):\n",
      " |              Specific download configuration parameters.\n",
      " |          download_mode ([`DownloadMode`] or `str`, *optional*):\n",
      " |              Select the download/generate mode, default to `REUSE_DATASET_IF_EXISTS`.\n",
      " |          verification_mode ([`VerificationMode`] or `str`, defaults to `BASIC_CHECKS`):\n",
      " |              Verification mode determining the checks to run on the downloaded/processed dataset information (checksums/size/splits/...).\n",
      " |\n",
      " |              <Added version=\"2.9.1\"/>\n",
      " |          dl_manager (`DownloadManager`, *optional*):\n",
      " |              Specific `DownloadManger` to use.\n",
      " |          base_path (`str`, *optional*):\n",
      " |              Base path for relative paths that are used to download files. This can be a remote url.\n",
      " |              If not specified, the value of the `base_path` attribute (`self.base_path`) will be used instead.\n",
      " |          file_format (`str`, *optional*):\n",
      " |              Format of the data files in which the dataset will be written.\n",
      " |              Supported formats: \"arrow\", \"parquet\". Default to \"arrow\" format.\n",
      " |              If the format is \"parquet\", then image and audio data are embedded into the Parquet files instead of pointing to local files.\n",
      " |\n",
      " |              <Added version=\"2.5.0\"/>\n",
      " |          max_shard_size (`Union[str, int]`, *optional*):\n",
      " |              Maximum number of bytes written per shard, default is \"500MB\".\n",
      " |              The size is based on uncompressed data size, so in practice your shard files may be smaller than\n",
      " |              `max_shard_size` thanks to Parquet compression for example.\n",
      " |\n",
      " |              <Added version=\"2.5.0\"/>\n",
      " |          num_proc (`int`, *optional*, defaults to `None`):\n",
      " |              Number of processes when downloading and generating the dataset locally.\n",
      " |              Multiprocessing is disabled by default.\n",
      " |\n",
      " |              <Added version=\"2.7.0\"/>\n",
      " |          storage_options (`dict`, *optional*):\n",
      " |              Key/value pairs to be passed on to the caching file-system backend, if any.\n",
      " |\n",
      " |              <Added version=\"2.5.0\"/>\n",
      " |          **download_and_prepare_kwargs (additional keyword arguments): Keyword arguments.\n",
      " |\n",
      " |      Example:\n",
      " |\n",
      " |      Download and prepare the dataset as Arrow files that can be loaded as a Dataset using `builder.as_dataset()`:\n",
      " |\n",
      " |      ```py\n",
      " |      >>> from datasets import load_dataset_builder\n",
      " |      >>> builder = load_dataset_builder(\"cornell-movie-review-data/rotten_tomatoes\")\n",
      " |      >>> builder.download_and_prepare()\n",
      " |      ```\n",
      " |\n",
      " |      Download and prepare the dataset as sharded Parquet files locally:\n",
      " |\n",
      " |      ```py\n",
      " |      >>> from datasets import load_dataset_builder\n",
      " |      >>> builder = load_dataset_builder(\"cornell-movie-review-data/rotten_tomatoes\")\n",
      " |      >>> builder.download_and_prepare(\"./output_dir\", file_format=\"parquet\")\n",
      " |      ```\n",
      " |\n",
      " |      Download and prepare the dataset as sharded Parquet files in a cloud storage:\n",
      " |\n",
      " |      ```py\n",
      " |      >>> from datasets import load_dataset_builder\n",
      " |      >>> storage_options = {\"key\": aws_access_key_id, \"secret\": aws_secret_access_key}\n",
      " |      >>> builder = load_dataset_builder(\"cornell-movie-review-data/rotten_tomatoes\")\n",
      " |      >>> builder.download_and_prepare(\"s3://my-bucket/my_rotten_tomatoes\", storage_options=storage_options, file_format=\"parquet\")\n",
      " |      ```\n",
      " |\n",
      " |  download_post_processing_resources(self, dl_manager)\n",
      " |\n",
      " |  get_exported_dataset_info(self) -> datasets.info.DatasetInfo\n",
      " |      Empty `DatasetInfo` if doesn't exist\n",
      " |\n",
      " |      Example:\n",
      " |\n",
      " |      ```py\n",
      " |      >>> from datasets import load_dataset_builder\n",
      " |      >>> ds_builder = load_dataset_builder('cornell-movie-review-data/rotten_tomatoes')\n",
      " |      >>> ds_builder.get_exported_dataset_info()\n",
      " |      DatasetInfo(description='', citation='', homepage='', license='', features={'speaker_id': Value(dtype='string', id=None), 'path': Value(dtype='string', id=None), 'audio': Audio(sampling_rate=16000, mono=True, decode=True, id=None), 'sentence': Value(dtype='string', id=None)}, post_processed=None, supervised_keys=None, builder_name=None, dataset_name=None, config_name='default', version=None, splits={'train': SplitInfo(name='train', num_bytes=1722002133, num_examples=11660, shard_lengths=None, dataset_name=None), 'test': SplitInfo(name='test', num_bytes=86120227, num_examples=760, shard_lengths=None, dataset_name=None)}, download_checksums=None, download_size=1475540500, post_processing_size=None, dataset_size=1808122360, size_in_bytes=None)\n",
      " |      ```\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from datasets.builder.DatasetBuilder:\n",
      " |\n",
      " |  get_all_exported_dataset_infos() -> datasets.info.DatasetInfosDict\n",
      " |      Empty dict if doesn't exist\n",
      " |\n",
      " |      Example:\n",
      " |\n",
      " |      ```py\n",
      " |      >>> from datasets import load_dataset_builder\n",
      " |      >>> ds_builder = load_dataset_builder('vivos')\n",
      " |      >>> ds_builder.get_all_exported_dataset_infos()\n",
      " |      {'default': DatasetInfo(description='', citation='', homepage='', license='', features={'speaker_id': Value(dtype='string', id=None), 'path': Value(dtype='string', id=None), 'audio': Audio(sampling_rate=16000, mono=True, decode=True, id=None), 'sentence': Value(dtype='string', id=None)}, post_processed=None, supervised_keys=None, builder_name=None, dataset_name=None, config_name='default', version=None, splits={'train': SplitInfo(name='train', num_bytes=1722002133, num_examples=11660, shard_lengths=None, dataset_name=None), 'test': SplitInfo(name='test', num_bytes=86120227, num_examples=760, shard_lengths=None, dataset_name=None)}, download_checksums=None, download_size=1475540500, post_processing_size=None, dataset_size=1808122360, size_in_bytes=None)}\n",
      " |      ```\n",
      " |\n",
      " |  get_imported_module_dir()\n",
      " |      Return the path of the module of this class or subclass.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from datasets.builder.DatasetBuilder:\n",
      " |\n",
      " |  builder_configs\n",
      " |      Dictionary of pre-defined configurations for this builder class.\n",
      " |\n",
      " |  cache_dir\n",
      " |\n",
      " |  manual_download_instructions\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from datasets.builder.DatasetBuilder:\n",
      " |\n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |\n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from datasets.builder.DatasetBuilder:\n",
      " |\n",
      " |  BUILDER_CONFIG_CLASS = <class 'datasets.builder.BuilderConfig'>\n",
      " |      Base class for `DatasetBuilder` data configuration.\n",
      " |\n",
      " |      `DatasetBuilder` subclasses with data configuration options should subclass\n",
      " |      `BuilderConfig` and add their own properties.\n",
      " |\n",
      " |      Attributes:\n",
      " |          name (`str`, defaults to `default`):\n",
      " |              The name of the configuration.\n",
      " |          version (`Version` or `str`, defaults to `0.0.0`):\n",
      " |              The version of the configuration.\n",
      " |          data_dir (`str`, *optional*):\n",
      " |              Path to the directory containing the source data.\n",
      " |          data_files (`str` or `Sequence` or `Mapping`, *optional*):\n",
      " |              Path(s) to source data file(s).\n",
      " |          description (`str`, *optional*):\n",
      " |              A human description of the configuration.\n",
      " |\n",
      " |\n",
      " |  DEFAULT_WRITER_BATCH_SIZE = None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(builder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7baa0e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] number of samples 1880853\n",
      "[validation] number of samples 89154\n",
      "[test] number of samples 100529\n"
     ]
    }
   ],
   "source": [
    "splits = [\"train\", \"validation\", \"test\"]\n",
    "\n",
    "datasets = {}\n",
    "\n",
    "for split in splits:\n",
    "    datasets[split] = builder.as_dataset(split=split) # type: ignore\n",
    "    print(f\"[{split}] number of samples {len(datasets[split])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8547cfc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def train(train_dir, model_save_path=None, n_neighbors=None, knn_algo=\\'ball_tree\\', verbose=False):\\n    \"\"\"\\n    Trains a k-nearest neighbors classifier for face recognition.\\n\\n    :param train_dir: directory that contains a sub-directory for each known person, with its name.\\n\\n     (View in source code to see train_dir example tree structure)\\n\\n     Structure:\\n        <train_dir>/\\n        ├── <person1>/\\n        │   ├── <somename1>.jpeg\\n        │   ├── <somename2>.jpeg\\n        │   ├── ...\\n        ├── <person2>/\\n        │   ├── <somename1>.jpeg\\n        │   └── <somename2>.jpeg\\n        └── ...\\n\\n    :param model_save_path: (optional) path to save model on disk\\n    :param n_neighbors: (optional) number of neighbors to weigh in classification. Chosen automatically if not specified\\n    :param knn_algo: (optional) underlying data structure to support knn.default is ball_tree\\n    :param verbose: verbosity of training\\n    :return: returns knn classifier that was trained on the given data.\\n    \"\"\"\\n    X = []\\n    y = []\\n\\n    # Loop through each person in the training set\\n    for class_dir in os.listdir(train_dir):\\n        if not os.path.isdir(os.path.join(train_dir, class_dir)):\\n            continue\\n\\n        # Loop through each training image for the current person\\n        for img_path in image_files_in_folder(os.path.join(train_dir, class_dir)):\\n            image = face_recognition.load_image_file(img_path)\\n            face_bounding_boxes = face_recognition.face_locations(image)\\n\\n            if len(face_bounding_boxes) != 1:\\n                # If there are no people (or too many people) in a training image, skip the image.\\n                if verbose:\\n                    print(\"Image {} not suitable for training: {}\".format(img_path, \"Didn\\'t find a face\" if len(face_bounding_boxes) < 1 else \"Found more than one face\"))\\n            else:\\n                # Add face encoding for current image to the training set\\n                X.append(face_recognition.face_encodings(image, known_face_locations=face_bounding_boxes)[0])\\n                y.append(class_dir)\\n\\n    # Determine how many neighbors to use for weighting in the KNN classifier\\n    if n_neighbors is None:\\n        n_neighbors = int(round(math.sqrt(len(X))))\\n        if verbose:\\n            print(\"Chose n_neighbors automatically:\", n_neighbors)\\n\\n    # Create and train the KNN classifier\\n    knn_clf = neighbors.KNeighborsClassifier(n_neighbors=n_neighbors, algorithm=knn_algo, weights=\\'distance\\')\\n    knn_clf.fit(X, y)\\n\\n    # Save the trained KNN classifier\\n    if model_save_path is not None:\\n        with open(model_save_path, \\'wb\\') as f:\\n            pickle.dump(knn_clf, f)\\n\\n    return knn_clf'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets['train'][0][\"func_code_string\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "545e449c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 1880853/1880853 [11:11<00:00, 2802.87 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] filtered number of samples 412178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 89154/89154 [00:29<00:00, 3033.63 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[validation] filtered number of samples 23107\n",
      "[test] filtered number of samples 22176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# keep only python samples\n",
    "filtered_datasets = {}\n",
    "for split in splits:\n",
    "    filtered_datasets[split] = datasets[split].filter(lambda example: example[\"language\"] == \"python\")\n",
    "    print(f\"[{split}] filtered number of samples {len(filtered_datasets[split])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fcae4b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 412178/412178 [03:03<00:00, 2248.53 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] filtered number of samples by size 412178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 23107/23107 [00:09<00:00, 2328.82 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[validation] filtered number of samples by size 23107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 22176/22176 [00:09<00:00, 2293.33 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[test] filtered number of samples by size 22176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "filtered_datasets_by_size = {}\n",
    "MAX_LENGTH = 100\n",
    "for split in splits:\n",
    "    filtered_datasets_by_size[split] = filtered_datasets[split].filter(lambda example: len(example[\"func_code_tokens\"]) <= MAX_LENGTH)\n",
    "    print(f\"[{split}] filtered number of samples by size {len(filtered_datasets_by_size[split])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1f7a5526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] sampled 10000\n",
      "[validation] sampled 2000\n",
      "[test] sampled 2000\n"
     ]
    }
   ],
   "source": [
    "import random \n",
    "K = [10000, 2000, 2000] # train, val, test \n",
    "sampled_data = {}\n",
    "\n",
    "for k_sample, split in zip(K,splits):\n",
    "    indices = random.sample(range(len(filtered_datasets_by_size[split])), k=k_sample)\n",
    "    sampled_data[split]= filtered_datasets_by_size[split].select(indices)\n",
    "    print(f\"[{split}] sampled {len(sampled_data[split])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a625bbee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'repository_name': 'pydata/xarray',\n",
       " 'func_path_in_repository': 'xarray/core/accessors.py',\n",
       " 'func_name': '_access_through_cftimeindex',\n",
       " 'whole_func_string': 'def _access_through_cftimeindex(values, name):\\n    \"\"\"Coerce an array of datetime-like values to a CFTimeIndex\\n    and access requested datetime component\\n    \"\"\"\\n    from ..coding.cftimeindex import CFTimeIndex\\n    values_as_cftimeindex = CFTimeIndex(values.ravel())\\n    if name == \\'season\\':\\n        months = values_as_cftimeindex.month\\n        field_values = _season_from_months(months)\\n    else:\\n        field_values = getattr(values_as_cftimeindex, name)\\n    return field_values.reshape(values.shape)',\n",
       " 'language': 'python',\n",
       " 'func_code_string': 'def _access_through_cftimeindex(values, name):\\n    \"\"\"Coerce an array of datetime-like values to a CFTimeIndex\\n    and access requested datetime component\\n    \"\"\"\\n    from ..coding.cftimeindex import CFTimeIndex\\n    values_as_cftimeindex = CFTimeIndex(values.ravel())\\n    if name == \\'season\\':\\n        months = values_as_cftimeindex.month\\n        field_values = _season_from_months(months)\\n    else:\\n        field_values = getattr(values_as_cftimeindex, name)\\n    return field_values.reshape(values.shape)',\n",
       " 'func_code_tokens': ['def',\n",
       "  '_access_through_cftimeindex',\n",
       "  '(',\n",
       "  'values',\n",
       "  ',',\n",
       "  'name',\n",
       "  ')',\n",
       "  ':',\n",
       "  'from',\n",
       "  '.',\n",
       "  '.',\n",
       "  'coding',\n",
       "  '.',\n",
       "  'cftimeindex',\n",
       "  'import',\n",
       "  'CFTimeIndex',\n",
       "  'values_as_cftimeindex',\n",
       "  '=',\n",
       "  'CFTimeIndex',\n",
       "  '(',\n",
       "  'values',\n",
       "  '.',\n",
       "  'ravel',\n",
       "  '(',\n",
       "  ')',\n",
       "  ')',\n",
       "  'if',\n",
       "  'name',\n",
       "  '==',\n",
       "  \"'season'\",\n",
       "  ':',\n",
       "  'months',\n",
       "  '=',\n",
       "  'values_as_cftimeindex',\n",
       "  '.',\n",
       "  'month',\n",
       "  'field_values',\n",
       "  '=',\n",
       "  '_season_from_months',\n",
       "  '(',\n",
       "  'months',\n",
       "  ')',\n",
       "  'else',\n",
       "  ':',\n",
       "  'field_values',\n",
       "  '=',\n",
       "  'getattr',\n",
       "  '(',\n",
       "  'values_as_cftimeindex',\n",
       "  ',',\n",
       "  'name',\n",
       "  ')',\n",
       "  'return',\n",
       "  'field_values',\n",
       "  '.',\n",
       "  'reshape',\n",
       "  '(',\n",
       "  'values',\n",
       "  '.',\n",
       "  'shape',\n",
       "  ')'],\n",
       " 'func_documentation_string': 'Coerce an array of datetime-like values to a CFTimeIndex\\n    and access requested datetime component',\n",
       " 'func_documentation_tokens': ['Coerce',\n",
       "  'an',\n",
       "  'array',\n",
       "  'of',\n",
       "  'datetime',\n",
       "  '-',\n",
       "  'like',\n",
       "  'values',\n",
       "  'to',\n",
       "  'a',\n",
       "  'CFTimeIndex',\n",
       "  'and',\n",
       "  'access',\n",
       "  'requested',\n",
       "  'datetime',\n",
       "  'component'],\n",
       " 'split_name': 'train',\n",
       " 'func_code_url': 'https://github.com/pydata/xarray/blob/6d93a95d05bdbfc33fff24064f67d29dd891ab58/xarray/core/accessors.py#L17-L28'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_data['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "310a58ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] cleaned 10000\n",
      "[validation] cleaned 2000\n",
      "[test] cleaned 2000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "keep = [\"func_code_string\",\"func_code_url\",\"split_name\"]\n",
    "\n",
    "data_selected_columns = {}\n",
    "\n",
    "for split in splits:\n",
    "    data_selected_columns[split] = sampled_data[split].select_columns(keep)\n",
    "    print(f\"[{split}] cleaned {len(data_selected_columns[split])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0502290",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "26851c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for split in splits:\n",
    "    dfs.append(data_selected_columns[split].to_pandas())\n",
    "dfs = pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d8353fda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>func_code_string</th>\n",
       "      <th>func_code_url</th>\n",
       "      <th>split_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>def _access_through_cftimeindex(values, name):...</td>\n",
       "      <td>https://github.com/pydata/xarray/blob/6d93a95d...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>def main():\\n    \"\"\"Start the bot.\"\"\"\\n    # B...</td>\n",
       "      <td>https://github.com/balemessenger/bale-bot-pyth...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>def insert(self, nodes, pos):\\n        # TODO:...</td>\n",
       "      <td>https://github.com/openego/ding0/blob/e2d6528f...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>def auth_complete(self, *args, **kwargs):\\n   ...</td>\n",
       "      <td>https://github.com/troeger/opensubmit/blob/384...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>def open(filename, frame='unspecified'):\\n    ...</td>\n",
       "      <td>https://github.com/BerkeleyAutomation/autolab_...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    func_code_string  \\\n",
       "0  def _access_through_cftimeindex(values, name):...   \n",
       "1  def main():\\n    \"\"\"Start the bot.\"\"\"\\n    # B...   \n",
       "2  def insert(self, nodes, pos):\\n        # TODO:...   \n",
       "3  def auth_complete(self, *args, **kwargs):\\n   ...   \n",
       "4  def open(filename, frame='unspecified'):\\n    ...   \n",
       "\n",
       "                                       func_code_url split_name  \n",
       "0  https://github.com/pydata/xarray/blob/6d93a95d...      train  \n",
       "1  https://github.com/balemessenger/bale-bot-pyth...      train  \n",
       "2  https://github.com/openego/ding0/blob/e2d6528f...      train  \n",
       "3  https://github.com/troeger/opensubmit/blob/384...      train  \n",
       "4  https://github.com/BerkeleyAutomation/autolab_...      train  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "397a1736",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs[dfs == 'valid'] = 'val'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c5b24ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs.to_parquet(\"dataset.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
